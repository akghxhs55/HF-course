{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102],\n",
      "        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]])\n",
      "[[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], [101, 1045, 5223, 2023, 2061, 2172, 999, 102]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequences = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much!\",\n",
    "]\n",
    "\n",
    "batched_ids = []\n",
    "max_len = 0\n",
    "\n",
    "for sequence in sequences:\n",
    "    tokens = tokenizer.tokenize(sequence)\n",
    "    ids = [101] + tokenizer.convert_tokens_to_ids(tokens) + [102]\n",
    "\n",
    "    batched_ids.append(ids)\n",
    "    max_len = max(max_len, len(ids))\n",
    "\n",
    "print(tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")[\"input_ids\"])\n",
    "print(batched_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5607,  1.6123],\n",
      "        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "padded_ids = []\n",
    "attention_mask = []\n",
    "for ids in batched_ids:\n",
    "    padded_ids.append(ids + [0] * (max_len - len(ids)))\n",
    "    attention_mask.append([1] * len(ids) + [0] * (max_len - len(ids)))\n",
    "\n",
    "outputs = model(torch.tensor(padded_ids), attention_mask=torch.tensor(attention_mask))\n",
    "\n",
    "print(outputs.logits)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e887e53ff547f858baab7c0b0222d3dddd4538743dd36791a8e77924af8e8896"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
